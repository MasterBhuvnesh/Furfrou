# ğŸ“˜ Light Novel AI Agent â€” System Design

## ğŸ¯ Objective

Build a **LangChain + Ollama powered AI Agent** that can:

- Answer questions from a **10-volume light novel**
- Use **retrieval-augmented generation (RAG)**
- Support **dynamic volume uploads**
- Use **DeepSeek R1 via Ollama**
- Store embeddings in a **local vector database**

---

## ğŸ§  System Overview

The system will process novels, embed them, store them in a vector database, and allow an AI agent to retrieve relevant content when answering user queries.

---

## ğŸ—ï¸ High-Level Architecture

```
Volumes (PDF/Text)
        â†“
Document Loaders
        â†“
Text Splitters
        â†“
Embedding Model
        â†“
Vector Store (ChromaDB)
        â†“
Retriever
        â†“
Agent (DeepSeek R1 via Ollama)
        â†“
Tools + Prompting + Memory
        â†“
Final Answer
```

---

## ğŸ“‚ Project Structure

```
project/
â”‚
â”œâ”€â”€ agent.py            # Main agent logic
â”œâ”€â”€ embedding.py        # Embedding model loader + logic
â”œâ”€â”€ vectorstore.py      # Vector DB storage + loading
â”œâ”€â”€ retriever.py        # Retrieval logic
â”œâ”€â”€ tools.py            # Agent tools
â”œâ”€â”€ prompting.py        # System prompt templates
â”œâ”€â”€ memory.py           # Conversation memory
â”œâ”€â”€ loaders.py          # Document loaders (PDF, TXT)
â”œâ”€â”€ splitter.py         # Text splitting logic
â”œâ”€â”€ ingest.py           # Data ingestion pipeline
â”œâ”€â”€ config.py           # Central configuration
â”œâ”€â”€ registry.json       # Tracks processed volumes
â”œâ”€â”€ main.py             # User interaction interface
â””â”€â”€ requirements.txt
```

---

## ğŸ—„ï¸ Vector Database Choice

**ChromaDB (Local)**

**Why:**

- Easy to use
- Persistent local storage
- Fully compatible with LangChain
- Efficient for novel-scale RAG

---

## ğŸ”¢ Embedding Model

Use one of the following (via Ollama):

| Model                | Purpose                             |
| -------------------- | ----------------------------------- |
| **nomic-embed-text** | Best general text embedding         |
| bge-large-en         | Alternative high-quality embeddings |
| mxbai-embed-large    | We are using this                   |

---

## ğŸ¤– LLM Model

| Role     | Model                    |
| -------- | ------------------------ |
| Main LLM | **DeepSeek R1 (Ollama)** |

This powers reasoning, agent decisions, and response generation.

---

## ğŸ§© Data Ingestion Flow

### Purpose:

Convert novel volumes into vector embeddings.

### Steps:

```
1. Load document using loaders.py
2. Split into chunks using splitter.py
3. Generate embeddings using embedding.py
4. Store embeddings in ChromaDB via vectorstore.py
5. Update registry.json to mark processed volumes
```

---

## ğŸ“„ Registry Logic

A `registry.json` file tracks what volumes have already been embedded.

Example:

```json
{
  "volume_1.pdf": {
    "status": "embedded",
    "chunks": 245,
    "last_updated": "2026-01-18"
  }
}
```

### Upload Handling Logic

```
If uploaded file not in registry â†’ Process + store + update registry
Else â†’ Skip ingestion
```

---

## ğŸ” Retrieval Flow

```
User Query â†’ Embed Query â†’ Similarity Search â†’ Retrieve Relevant Chunks â†’ Send to LLM
```

Handled by:

- `retriever.py`
- `vectorstore.py`
- `embedding.py`

---

## ğŸ§° Agent Tools

Defined in `tools.py`

| Tool           | Function                              |
| -------------- | ------------------------------------- |
| Retriever Tool | Fetch relevant novel context          |
| Character Tool | Extract character-related passages    |
| Volume Finder  | Identify which volume contains events |
| Summarizer     | Condense long retrieved text          |
| Timeline Tool  | Understand story progression          |

---

## ğŸ§  Agent Capabilities

The agent will:

- Use RAG to fetch relevant text
- Decide which tool to use for a question
- Maintain memory for conversations
- Answer contextually based on all stored volumes

---

## ğŸ’¬ Prompting Strategy

Handled by `prompting.py`

Includes:

- Agent identity
- Novel context awareness
- Tool usage guidelines
- Response formatting instructions

---

## ğŸ§  Memory

Handled by `memory.py`

Supports:

- Chat history buffer
- Context retention across user sessions

---

## ğŸš€ Deployment Options

- CLI via `main.py`
- Web API via FastAPI (future upgrade)
- Extendable to serverless or cloud

---

## ğŸ§© Final Workflow

```
Load & Ingest Volumes â†’ Store in Vector DB â†’ Start Agent â†’ Ask Questions â†’ Agent Retrieves + Answers â†’ Add New Volumes Anytime
```

---

## âœ… System Ready For Implementation

This design supports:

- Scalability
- Clean modular coding
- Future multi-agent or API upgrades
