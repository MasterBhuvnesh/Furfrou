{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26cdc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent for content generation using AWS Bedrock\n",
    "# AI agent will save the content in a database\n",
    "# AI agent will generate content based on the data a lambda function will pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ca2dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration AWS\n",
    "BEDROCK_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "BEDROCK_EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "AWS_REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdf6688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The benefits of using AWS cloud services include scalability, cost-effectiveness, reliability, security, and a wide range of services and tools to support various business needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing AWS Client\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "test_llm = ChatBedrock(\n",
    "    model_id = BEDROCK_MODEL_ID,\n",
    "    region_name = AWS_REGION,\n",
    "    model_kwargs = {\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    ")\n",
    "test_response = test_llm.invoke(\"What are the benefits of using AWS cloud services? in 1 sentence only.\")\n",
    "print(\"Response:\", test_response.content)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85339299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring ChromaDB & Others\n",
    "\n",
    "# ChromaDB persistent storage path\n",
    "CHROMA_DB_PATH = \"./vector_db\"\n",
    "\n",
    "# PDF files directory\n",
    "PDF_DATA_PATH = \"./.docs\"\n",
    "\n",
    "# Text splitting configuration\n",
    "# Size of each text chunk\n",
    "CHUNK_SIZE = 800      \n",
    "# Overlap between chunks for context continuity\n",
    "CHUNK_OVERLAP = 150    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc87ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "   LLM: anthropic.claude-3-haiku-20240307-v1:0\n",
      "   Embeddings: amazon.titan-embed-text-v2:0\n",
      "   Vector DB: ./vector_db\n",
      "   PDFs: ./.docs\n"
     ]
    }
   ],
   "source": [
    "#  Configuration Loading\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"   LLM: {BEDROCK_MODEL_ID}\")\n",
    "print(f\"   Embeddings: {BEDROCK_EMBEDDING_MODEL_ID}\")\n",
    "print(f\"   Vector DB: {CHROMA_DB_PATH}\")\n",
    "print(f\"   PDFs: {PDF_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa5933fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF documents...\n",
      "   Loading: Activation Functions.pdf\n",
      "Loaded 10 pages from PDFs\n"
     ]
    }
   ],
   "source": [
    "#  Loading PDFs files\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "\n",
    "def load_documents(path: str):\n",
    "    docs = []\n",
    "    \n",
    "    # Loop through every file in the directory\n",
    "    for file in os.listdir(path):\n",
    "        # Only process PDF files\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            print(f\"   Loading: {file}\")\n",
    "            \n",
    "            # PyPDFLoader extracts text from each page\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs.extend(loader.load())\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Load all PDFs\n",
    "print(\"Loading PDF documents...\")\n",
    "documents = load_documents(PDF_DATA_PATH)\n",
    "print(f\"Loaded {len(documents)} pages from PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8141bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks...\n",
      "Created 25 chunks\n",
      "   Sample chunk preview: Introduction to Activation Functionsin Neural Networks\n",
      "Gain an understanding of common activation fu...\n"
     ]
    }
   ],
   "source": [
    "# Splitting Text into Chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents: list):\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,        # Max characters per chunk\n",
    "        chunk_overlap=CHUNK_OVERLAP   # Overlapping chars between chunks\n",
    "    )\n",
    "\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# Split documents into chunks\n",
    "print(\"Splitting documents into chunks...\")\n",
    "chunks = split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"   Sample chunk preview: {chunks[0].page_content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd4b1644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and storing in ChromaDB...\n",
      "Vector store created at: ./vector_db\n"
     ]
    }
   ],
   "source": [
    "# Embedding + Vector Database\n",
    "\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def build_vector_store(chunks: list):\n",
    "    embedding = BedrockEmbeddings(\n",
    "        model_id=BEDROCK_EMBEDDING_MODEL_ID,\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    # Create ChromaDB vector store from documents\n",
    "    # This embeds all chunks and stores them persistently\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding,\n",
    "        persist_directory=CHROMA_DB_PATH\n",
    "    )\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Build and persist vector store\n",
    "print(\"Creating embeddings and storing in ChromaDB...\")\n",
    "vectorstore = build_vector_store(chunks)\n",
    "print(f\"Vector store created at: {CHROMA_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57117cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever...\n",
      "Retriever ready for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Retriever Setup\n",
    "\n",
    "def get_retriever():\n",
    "    # Load the persisted vector store\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_DB_PATH,\n",
    "        embedding_function=BedrockEmbeddings(\n",
    "        model_id=BEDROCK_EMBEDDING_MODEL_ID,\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    )\n",
    "    \n",
    "    # Create retriever that returns top 5 most similar chunks\n",
    "    return db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Initialize retriever\n",
    "print(\"Initializing retriever...\")\n",
    "retriever = get_retriever()\n",
    "print(\"Retriever ready for similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5508d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " System prompt configured for structured JSON output\n"
     ]
    }
   ],
   "source": [
    "# Structured JSON System Prompt\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Machine Learning  Assistant.\n",
    "Use the user input, retrieved PDF content and respond ONLY in valid JSON format.\n",
    "\n",
    "Your response MUST be a valid JSON object with this exact structure:\n",
    "\n",
    "{\n",
    "  \"title\": \"Topic title based on the question\",\n",
    "  \"content\": \"Detailed explanation from the PDF content\",\n",
    "  \"summary\": \"A concise 2-3 sentence summary\",\n",
    "  \"facts\": \"Key facts, figures, and specifications\",\n",
    "  \"quiz\": {\n",
    "    \"topic\": \"Quiz topic\",\n",
    "    \"questions\": [\n",
    "      {\n",
    "        \"question\": \"Question text\",\n",
    "        \"options\": [\"A) ...\", \"B) ...\", \"C) ...\", \"D) ...\"],\n",
    "        \"correct_answer\": \"A\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"key_notes\": {\n",
    "   \"1.\":\"points should be of 1 sentence,and should be in short\",\n",
    "   \"2.\":\"\",\n",
    "   \"3.\":\"\"\n",
    "  }\n",
    "}\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. ONLY output valid JSON - no markdown, no explanations outside JSON\n",
    "2. Base ALL content on the retrieved PDF context and the user query\n",
    "3. If information is not found, use null or empty arrays []\n",
    "4. Generate 2-3 quiz questions to test understanding\n",
    "5. Extract any formulas, definitions, and key terms\n",
    "\"\"\"\n",
    "\n",
    "print(\" System prompt configured for structured JSON output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f5d1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# AGENT SETUP\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "# PDF Retriever Tool\n",
    "@tool\n",
    "def retrieve_context(query: str) -> str:\n",
    "    \"\"\"Search technical PDF documents for relevant information about Machine Learning.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# Bedrock LLM \n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=BEDROCK_MODEL_ID,\n",
    "    region_name=AWS_REGION,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 10000,\n",
    "        \"temperature\": 0.4,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create Agent\n",
    "agent = create_agent(\n",
    "    model=bedrock_llm,\n",
    "    tools=[retrieve_context],\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"Agent initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Agent Query\n",
    "\n",
    "query = \"\"\"Activation functions are fundamental components of artificial neural networks that determine whether and how strongly a neuron should fire based on its input. They introduce non-linearity into the network, enabling it to learn complex patterns and solve problems that linear models cannot handle.\n",
    "The Role of Activation Functions\n",
    "In a neural network, each neuron receives weighted inputs, sums them together, and passes this sum through an activation function to produce an output. Without activation functions, neural networks would simply be stacked linear transformations, no matter how many layers they contain. The activation function is what allows networks to approximate complex, non-linear relationships in data.\n",
    "Common Activation Functions\n",
    "Several activation functions have been developed over the years, each with distinct characteristics. The sigmoid function, one of the earliest used, squashes input values between 0 and 1, making it useful for binary classification. However, it suffers from the vanishing gradient problem, where gradients become extremely small during backpropagation, slowing down learning in deep networks.\n",
    "The hyperbolic tangent (tanh) function addresses some sigmoid limitations by centering outputs around zero, ranging from -1 to 1. While this often leads to faster convergence, it still experiences vanishing gradients for extreme input values.\n",
    "The Rectified Linear Unit (ReLU) revolutionized deep learning when it became widely adopted. It simply outputs the input if positive, and zero otherwise. ReLU is computationally efficient and helps mitigate vanishing gradients, though it can suffer from \"dying ReLU\" where neurons become permanently inactive. Variants like Leaky ReLU and Parametric ReLU were developed to address this by allowing small negative values.\n",
    "More recent innovations include ELU (Exponential Linear Unit), which smooths the function for negative values, and Swish, a self-gated activation function developed by Google that has shown improved performance in some deep networks.\n",
    "Choosing the Right Activation Function\n",
    "The choice of activation function depends on the specific problem and network architecture. ReLU and its variants remain the default choice for hidden layers in most deep learning applications due to their effectiveness and computational efficiency. For output layers, the choice depends on the task: sigmoid for binary classification, softmax for multi-class classification, and linear activation for regression problems.\n",
    "Conclusion\n",
    "Activation functions are essential for giving neural networks their power and flexibility. As deep learning continues to evolve, researchers continue exploring new activation functions that can train faster, generalize better, and overcome the limitations of existing approaches. Understanding these functions is crucial for anyone working with neural networks, as they directly impact model performance and training dynamics.\"\"\"\n",
    "\n",
    "# print(f\"Query: {query}\")\n",
    "\n",
    "# Run the agent\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    ")\n",
    "\n",
    "\n",
    "# pprint(response['messages'][1][0], width=120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1aed539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Activation functions are fundamental components of artificial neural networks that determine whether and '\n",
      "            'how strongly a neuron should fire based on its input. They introduce non-linearity into the network, '\n",
      "            'enabling it to learn complex patterns and solve problems that linear models cannot handle. Without '\n",
      "            'activation functions, neural networks would simply be stacked linear transformations, no matter how many '\n",
      "            'layers they contain. The activation function is what allows networks to approximate complex, non-linear '\n",
      "            'relationships in data.\\n'\n",
      "            '\\n'\n",
      "            'Several activation functions have been developed over the years, each with distinct characteristics. The '\n",
      "            'sigmoid function squashes input values between 0 and 1, making it useful for binary classification, but '\n",
      "            'it suffers from the vanishing gradient problem. The hyperbolic tangent (tanh) function addresses some '\n",
      "            'sigmoid limitations by centering outputs around zero, ranging from -1 to 1, but it still experiences '\n",
      "            'vanishing gradients for extreme input values. The Rectified Linear Unit (ReLU) revolutionized deep '\n",
      "            'learning when it became widely adopted, as it is computationally efficient and helps mitigate vanishing '\n",
      "            'gradients, though it can suffer from \"dying ReLU\" where neurons become permanently inactive. Variants '\n",
      "            'like Leaky ReLU and Parametric ReLU were developed to address this. More recent innovations include ELU '\n",
      "            '(Exponential Linear Unit), which smooths the function for negative values, and Swish, a self-gated '\n",
      "            'activation function developed by Google that has shown improved performance in some deep networks.\\n'\n",
      "            '\\n'\n",
      "            'The choice of activation function depends on the specific problem and network architecture. ReLU and its '\n",
      "            'variants remain the default choice for hidden layers in most deep learning applications due to their '\n",
      "            'effectiveness and computational efficiency. For output layers, the choice depends on the task: sigmoid '\n",
      "            'for binary classification, softmax for multi-class classification, and linear activation for regression '\n",
      "            'problems.',\n",
      " 'facts': {'Activation functions': 'Determine whether and how strongly a neuron should fire based on its input',\n",
      "           'ELU': 'Exponential Linear Unit, smooths the function for negative values',\n",
      "           'ReLU': 'Revolutionized deep learning, outputs the input if positive and zero otherwise, computationally '\n",
      "                   'efficient and helps mitigate vanishing gradients, but can suffer from \"dying ReLU\"',\n",
      "           'Role of activation functions': 'Introduce non-linearity into the network, enabling it to learn complex '\n",
      "                                           'patterns and solve problems that linear models cannot handle',\n",
      "           'Sigmoid function': 'Squashes input values between 0 and 1, useful for binary classification, but suffers '\n",
      "                               'from vanishing gradient problem',\n",
      "           'Swish': 'A self-gated activation function developed by Google that has shown improved performance in some '\n",
      "                    'deep networks',\n",
      "           'Tanh function': 'Addresses some sigmoid limitations by centering outputs around zero, ranging from -1 to '\n",
      "                            '1, but still experiences vanishing gradients for extreme input values',\n",
      "           'Variants of ReLU': 'Leaky ReLU and Parametric ReLU were developed to address the \"dying ReLU\" problem'},\n",
      " 'key_notes': {'1.': 'Activation functions introduce non-linearity into neural networks, enabling them to learn '\n",
      "                     'complex patterns.',\n",
      "               '2.': 'Several activation functions have been developed, each with unique characteristics and '\n",
      "                     'trade-offs.',\n",
      "               '3.': 'The choice of activation function depends on the specific problem and network architecture.'},\n",
      " 'quiz': {'questions': [{'correct_answer': 'D',\n",
      "                         'options': ['A) To determine the strength of neuron firing based on input',\n",
      "                                     'B) To introduce non-linearity into the network',\n",
      "                                     'C) To enable the network to learn complex patterns',\n",
      "                                     'D) All of the above'],\n",
      "                         'question': 'What is the primary role of activation functions in neural networks?'},\n",
      "                        {'correct_answer': 'D',\n",
      "                         'options': ['A) Sigmoid', 'B) Tanh', 'C) ReLU', 'D) Both A and B'],\n",
      "                         'question': 'Which activation function is known to suffer from the vanishing gradient '\n",
      "                                     'problem?'},\n",
      "                        {'correct_answer': 'D',\n",
      "                         'options': ['A) Computational efficiency',\n",
      "                                     'B) Mitigation of vanishing gradients',\n",
      "                                     'C) Ability to handle complex non-linear relationships',\n",
      "                                     'D) Both A and B'],\n",
      "                         'question': 'What is the main advantage of the ReLU activation function?'}],\n",
      "          'topic': 'Activation Functions in Neural Networks'},\n",
      " 'summary': 'Activation functions are essential components of neural networks that introduce non-linearity, enabling '\n",
      "            'them to learn complex patterns. Several activation functions have been developed, each with unique '\n",
      "            'characteristics, and the choice depends on the specific problem and network architecture.',\n",
      " 'title': 'The Role and Types of Activation Functions in Neural Networks'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Get the AIMessage object\n",
    "ai_msg = response[\"messages\"][1]\n",
    "\n",
    "# Extract JSON string\n",
    "json_str = ai_msg.content\n",
    "\n",
    "# Convert string into dict\n",
    "data = json.loads(json_str)\n",
    "\n",
    "# Pretty print\n",
    "pprint(data, width=120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31979f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_id': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
      " 'stop_reason': 'end_turn',\n",
      " 'usage': {'cache_read_input_tokens': 0,\n",
      "           'cache_write_input_tokens': 0,\n",
      "           'completion_tokens': 1139,\n",
      "           'prompt_tokens': 1199,\n",
      "           'total_tokens': 2338}}\n"
     ]
    }
   ],
   "source": [
    "# Extract JSON string\n",
    "mdata = ai_msg.additional_kwargs\n",
    "\n",
    "# Pretty print\n",
    "pprint(mdata, width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fca9ac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation functions are fundamental components of artificial neural networks that determine whether and how strongly a neuron should fire based on its input. They introduce non-linearity into the network, enabling it to learn complex patterns and solve problems that linear models cannot handle. Without activation functions, neural networks would simply be stacked linear transformations, no matter how many layers they contain. The activation function is what allows networks to approximate complex, non-linear relationships in data.\n",
      "\n",
      "Several activation functions have been developed over the years, each with distinct characteristics. The sigmoid function squashes input values between 0 and 1, making it useful for binary classification, but it suffers from the vanishing gradient problem. The hyperbolic tangent (tanh) function addresses some sigmoid limitations by centering outputs around zero, ranging from -1 to 1, but it still experiences vanishing gradients for extreme input values. The Rectified Linear Unit (ReLU) revolutionized deep learning when it became widely adopted, as it is computationally efficient and helps mitigate vanishing gradients, though it can suffer from \"dying ReLU\" where neurons become permanently inactive. Variants like Leaky ReLU and Parametric ReLU were developed to address this. More recent innovations include ELU (Exponential Linear Unit), which smooths the function for negative values, and Swish, a self-gated activation function developed by Google that has shown improved performance in some deep networks.\n",
      "\n",
      "The choice of activation function depends on the specific problem and network architecture. ReLU and its variants remain the default choice for hidden layers in most deep learning applications due to their effectiveness and computational efficiency. For output layers, the choice depends on the task: sigmoid for binary classification, softmax for multi-class classification, and linear activation for regression problems.\n"
     ]
    }
   ],
   "source": [
    "# Checkinh format\n",
    "print(data['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
