{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üß† Technical PDF Knowledge Agent\n",
    "\n",
    "**Agent Architecture:**\n",
    "```\n",
    "PDF Files (Microcontrollers)\n",
    "        ‚Üì\n",
    "Document Loader\n",
    "        ‚Üì\n",
    "Text Splitter\n",
    "        ‚Üì\n",
    "Embedding Model\n",
    "        ‚Üì\n",
    "Vector Store (ChromaDB)\n",
    "        ‚Üì\n",
    "Retriever\n",
    "        ‚Üì\n",
    "Agent (DeepSeek R1 via Ollama)\n",
    "        ‚Üì\n",
    "Structured JSON Output\n",
    "```\n",
    "\n",
    "This agent reads from technical PDFs and returns structured JSON with:\n",
    "- `title`, `content`, `summary`, `facts`, `quiz`, `key_notes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 1 ‚Äî Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# LLM Model: DeepSeek R1 via Ollama for reasoning\n",
    "OLLAMA_LLM_MODEL = \"deepseek-r1\"\n",
    "\n",
    "# Embedding Model: mxbai-embed-large for semantic search\n",
    "OLLAMA_EMBED_MODEL = \"mxbai-embed-large\"\n",
    "\n",
    "# ChromaDB persistent storage path\n",
    "CHROMA_DB_PATH = \"./vector_db\"\n",
    "\n",
    "# PDF files directory\n",
    "PDF_DATA_PATH = \"./.docs\"\n",
    "\n",
    "# Text splitting configuration\n",
    "CHUNK_SIZE = 800       # Size of each text chunk\n",
    "CHUNK_OVERLAP = 150    # Overlap between chunks for context continuity\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   LLM: {OLLAMA_LLM_MODEL}\")\n",
    "print(f\"   Embeddings: {OLLAMA_EMBED_MODEL}\")\n",
    "print(f\"   Vector DB: {CHROMA_DB_PATH}\")\n",
    "print(f\"   PDFs: {PDF_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2 ‚Äî Load PDFs\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "def load_documents(path: str):\n",
    "    \"\"\"\n",
    "    Load all PDF files from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        path: Directory containing PDF files\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects from all PDFs\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    # Loop through every file in the directory\n",
    "    for file in os.listdir(path):\n",
    "        # Only process PDF files\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            print(f\"   Loading: {file}\")\n",
    "            \n",
    "            # PyPDFLoader extracts text from each page\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs.extend(loader.load())\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Load all PDFs\n",
    "print(\"üìÑ Loading PDF documents...\")\n",
    "documents = load_documents(PDF_DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages from PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-splitter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3 ‚Äî Split Text into Chunks\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents: list):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for better embedding performance.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of Document objects\n",
    "        \n",
    "    Returns:\n",
    "        List of chunked Document objects\n",
    "    \"\"\"\n",
    "    # RecursiveCharacterTextSplitter tries to split on natural boundaries\n",
    "    # (paragraphs, sentences, words) before falling back to character count\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,        # Max characters per chunk\n",
    "        chunk_overlap=CHUNK_OVERLAP   # Overlapping chars between chunks\n",
    "    )\n",
    "    \n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# Split documents into chunks\n",
    "print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
    "chunks = split_documents(documents)\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "print(f\"   Sample chunk preview: {chunks[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-vectorstore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4 ‚Äî Embeddings + Vector Database\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def build_vector_store(chunks: list):\n",
    "    \"\"\"\n",
    "    Convert text chunks into embeddings and store in ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunked Document objects\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vector store instance\n",
    "    \"\"\"\n",
    "    # Initialize the embedding model (runs locally via Ollama)\n",
    "    embedding = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL)\n",
    "    \n",
    "    # Create ChromaDB vector store from documents\n",
    "    # This embeds all chunks and stores them persistently\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding,\n",
    "        persist_directory=CHROMA_DB_PATH\n",
    "    )\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Build and persist vector store\n",
    "print(\"üî¢ Creating embeddings and storing in ChromaDB...\")\n",
    "vectorstore = build_vector_store(chunks)\n",
    "print(f\"‚úÖ Vector store created at: {CHROMA_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5 ‚Äî Retriever Setup\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def get_retriever():\n",
    "    \"\"\"\n",
    "    Load existing vector store and create a retriever.\n",
    "    \n",
    "    Returns:\n",
    "        Retriever for similarity search over the vector store\n",
    "    \"\"\"\n",
    "    # Load the persisted vector store\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_DB_PATH,\n",
    "        embedding_function=OllamaEmbeddings(model=OLLAMA_EMBED_MODEL)\n",
    "    )\n",
    "    \n",
    "    # Create retriever that returns top 5 most similar chunks\n",
    "    return db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Initialize retriever\n",
    "print(\"üîç Initializing retriever...\")\n",
    "retriever = get_retriever()\n",
    "print(\"‚úÖ Retriever ready for similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6 ‚Äî Structured JSON System Prompt\n",
    "# ============================================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Microcontroller Learning Assistant.\n",
    "Use the retrieved PDF content and respond ONLY in valid JSON format.\n",
    "\n",
    "Your response MUST be a valid JSON object with this exact structure:\n",
    "\n",
    "{\n",
    "  \"title\": \"Topic title based on the question\",\n",
    "  \"content\": \"Detailed explanation from the PDF content\",\n",
    "  \"summary\": \"A concise 2-3 sentence summary\",\n",
    "  \"facts\": \"Key facts, figures, and specifications\",\n",
    "  \"quiz\": {\n",
    "    \"topic\": \"Quiz topic\",\n",
    "    \"difficulty\": \"easy/medium/hard\",\n",
    "    \"questions\": [\n",
    "      {\n",
    "        \"question\": \"Question text\",\n",
    "        \"options\": [\"A) ...\", \"B) ...\", \"C) ...\", \"D) ...\"],\n",
    "        \"correct_answer\": \"A\",\n",
    "        \"explanation\": \"Why this is correct\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"key_notes\": {\n",
    "    \"main_points\": [\"Point 1\", \"Point 2\"],\n",
    "    \"definitions\": [{\"term\": \"...\", \"definition\": \"...\"}],\n",
    "    \"formulas\": [\"Formula 1\", \"Formula 2\"],\n",
    "    \"diagrams_mentioned\": [\"Diagram descriptions if any\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. ONLY output valid JSON - no markdown, no explanations outside JSON\n",
    "2. Base ALL content on the retrieved PDF context\n",
    "3. If information is not found, use null or empty arrays []\n",
    "4. Generate 2-3 quiz questions to test understanding\n",
    "5. Extract any formulas, definitions, and key terms\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù System prompt configured for structured JSON output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7 ‚Äî Agent Setup with RAG + Memory\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def retrieve_context(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the vector store.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question or search query\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated content from relevant documents\n",
    "    \"\"\"\n",
    "    # Get relevant documents using similarity search\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    # Combine all document contents with separators\n",
    "    return \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# Create the PDF search tool\n",
    "pdf_search_tool = Tool(\n",
    "    name=\"PDFSearch\",\n",
    "    func=retrieve_context,\n",
    "    description=\"Search technical PDF documents for microcontroller concepts, circuits, and specifications\"\n",
    ")\n",
    "\n",
    "# Initialize conversation memory for multi-turn chat\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Initialize the LLM (DeepSeek R1 via Ollama)\n",
    "llm = ChatOllama(\n",
    "    model=OLLAMA_LLM_MODEL,\n",
    "    temperature=0.3  # Lower for more focused/consistent output\n",
    ")\n",
    "\n",
    "# Create the conversational agent\n",
    "agent = initialize_agent(\n",
    "    tools=[pdf_search_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    agent_kwargs={\n",
    "        \"system_message\": SYSTEM_PROMPT\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Agent initialized!\")\n",
    "print(f\"   Model: {OLLAMA_LLM_MODEL}\")\n",
    "print(\"   Tools: PDFSearch\")\n",
    "print(\"   Memory: ConversationBufferMemory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8 ‚Äî Run Agent Query\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Example query - change this to ask about any microcontroller topic\n",
    "query = \"Explain ADC in microcontrollers\"\n",
    "\n",
    "print(f\"‚ùì Query: {query}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run the agent\n",
    "response = agent.invoke({\"input\": query})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì§ Agent Response:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to parse and pretty-print JSON response\n",
    "try:\n",
    "    output = response[\"output\"]\n",
    "    \n",
    "    # Try to extract JSON from response\n",
    "    if \"{\" in output and \"}\" in output:\n",
    "        # Find JSON boundaries\n",
    "        start = output.find(\"{\")\n",
    "        end = output.rfind(\"}\") + 1\n",
    "        json_str = output[start:end]\n",
    "        \n",
    "        # Parse and pretty print\n",
    "        parsed = json.loads(json_str)\n",
    "        print(json.dumps(parsed, indent=2))\n",
    "    else:\n",
    "        print(output)\n",
    "except json.JSONDecodeError:\n",
    "    # If JSON parsing fails, print raw response\n",
    "    print(response[\"output\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9 ‚Äî Interactive Chat (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "def chat(query: str):\n",
    "    \"\"\"\n",
    "    Send a query to the agent and get a structured response.\n",
    "    \n",
    "    Args:\n",
    "        query: Your question about microcontrollers\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    response = agent.invoke({\"input\": query})\n",
    "    output = response[\"output\"]\n",
    "    \n",
    "    try:\n",
    "        if \"{\" in output:\n",
    "            start = output.find(\"{\")\n",
    "            end = output.rfind(\"}\") + 1\n",
    "            parsed = json.loads(output[start:end])\n",
    "            print(json.dumps(parsed, indent=2))\n",
    "        else:\n",
    "            print(output)\n",
    "    except:\n",
    "        print(output)\n",
    "\n",
    "# Example usage:\n",
    "# chat(\"What are the different types of memory in microcontrollers?\")\n",
    "# chat(\"Explain PWM and its applications\")\n",
    "# chat(\"How does I2C communication work?\")\n",
    "\n",
    "print(\"üí° Use chat('your question') to interact with the agent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
